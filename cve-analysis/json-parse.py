#!/usr/bin/env python3

import sys
import json
import os
import urllib.request
import elasticsearch
import elasticsearch.helpers
from elasticsearch import Elasticsearch

if 'ESURL' not in os.environ:
    es_url = "http://elastic:changeme@localhost:9200"
else:
    es_url = os.environ['ESURL']

# connect to Elasticsearch host
# retry_on_timeout is used to retry on timeout
es = Elasticsearch([es_url], retry_on_timeout=True)


class CVE:

    def __init__(self):
        self.ids = []
        self.current = -1
        self.rh_data = None

    def add(self, i):
        cve = i['cve']
        cve_id = cve['CVE_data_meta']['ID']
        cve.update(i['impact'])
        cve['year'] = cve_id.split('-')[1]
        cve['just_id'] = cve_id.split('-')[2]
        vuln = i['configurations']
        cve['vuln'] = vuln

        rh_data = self.__get_redhat_data(cve_id)
        if 'cvss3' in rh_data:
            cve['rh_cvssv3'] = rh_data['cvss3']
        if 'cvss2' in rh_data:
            cve['rh_cvssv2'] = rh_data['cvss2']
        if 'impact' in rh_data:
            cve['rh_impact'] = rh_data['impact']

        # Bulk inserting is a way to add multiple documents to Elasticsearch in a single request
        cve_bulk = {
                    # the action is update
                    "_op_type": "update",
                    "_index":   "cve-index",
                    "_id":      cve_id,
                    # doc_as_upsert is used to update a document
                    "doc_as_upsert": True,
                    "doc":  cve
                   }
        # append new bulk to ids
        self.ids.append(cve_bulk)

    def __next__(self):
        "Handle a call to next()"

        self.current = self.current + 1
        if self.current >= len(self.ids):
            raise StopIteration

        return self.ids[self.current]

    def __iter__(self):
        return self

    def __len__(self):
        return len(self.ids)

    def __get_redhat_data(self, cve_id):

        if self.rh_data is None:

            self.rh_data = {}

            fh = open('data/cve_dates.txt')
            for line in fh.readlines():
                line = line.rstrip()

                # The data format looks like
                # CVE key=value,key=value,...
                split_line = line.split(' ')
                cve = split_line[0]
                self.rh_data[cve] = {}

                if len(split_line) > 1:
                    # There are a few CVE IDs that don't have any data
                    data = split_line[1]
                else:
                    next

                for keyval in data.split(','):
                    (key, value) = keyval.split('=')
                    if key == 'cvss3' or key == 'cvss2':
                        # The cvss scores are special, we only want the
                        # number
                        value = float(value.split('/')[0])
                    self.rh_data[cve][key] = value

        if cve_id in self.rh_data:
            return self.rh_data[cve_id]
        else:
            return {}

def main():

    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        print("Usage: %s <nvd-xml-file>" % (sys.argv[0]))
        sys.exit(1)

    # First let's see if the index exists
    if es.indices.exists('cve-index') is False:
        # We have to create it and add a mapping
        fh = open('cve-index-json-mapping.json')
        # Mapping is the process of defining how a document, and the fields it contains, are stored and indexed
        mapping = json.load(fh)
        es.indices.create('cve-index', body=mapping)

    fh = open(input_file)
    json_data = json.load(fh)

    the_cves = CVE()

    # for each dictionary in CVE_Items list call the add(i) function to the_cves iterable object
    # i is the current dictionary
    for i in json_data['CVE_Items']:
        the_cves.add(i)

    # streaming_bulk(c) won't actually do anything. It's not until you iterate over it 
    # as is done in this for loop that the indexing actually starts to happen.
    # the the_cves is the iterable containing the actions to be executed
    # max_retries is the maximum number of times a document will be retried
    for ok, item in elasticsearch.helpers.streaming_bulk(es, the_cves, max_retries=9, request_timeout=30):
            if not ok:
                print("ERROR:")
                print(item)

if __name__ == "__main__":
    main()